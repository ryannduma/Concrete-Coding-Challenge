{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45132157",
   "metadata": {},
   "source": [
    "## Libraries and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b358c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0a6ac",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mervyn's Preprocessing - some complicated sh*t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# File path and variables\n",
    "filename = 'Concrete_Data_Yeh_final.csv'\n",
    "variables = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate', 'fineaggregate', 'age']\n",
    "\n",
    "# Preprocessing class\n",
    "class PreProcessing:\n",
    "    \"\"\"Handles data preprocessing tasks for regression modeling.\"\"\"\n",
    "\n",
    "    def __init__(self, file):\n",
    "        \"\"\"Initializes the class by reading the data from the file.\"\"\"\n",
    "        self.data = pd.read_csv(file)  # Read CSV file into a DataFrame\n",
    "\n",
    "    def checkNaN(self):\n",
    "        \"\"\"Checks for missing values (NaN) in the DataFrame.\"\"\"\n",
    "        return self.data.isnull().sum()  # Count missing values in each column\n",
    "\n",
    "    def FillNaN(self, method='mean'): #by default i've set it to use mean, but we can play around with KNN and Median to see how our model behaves with different imputation methods.\n",
    "        \"\"\"Fills missing values with specified method and applies transformations.\"\"\"\n",
    "        if method == 'mean':\n",
    "            # Fill NaNs with mean for Gaussian-distributed variables\n",
    "            for col in ['cement', 'water', 'coarseaggregate', 'fineaggregate']:\n",
    "                self.data[col].fillna(self.data[col].mean(), inplace=True)\n",
    "\n",
    "            # Handle variables with irregular Gaussian patterns\n",
    "            for col in ['slag', 'flyash', 'superplasticizer']:\n",
    "                mask = self.data[col] == 0\n",
    "                mean_csMPa_for_zeros = self.data[mask]['csMPa'].mean()  # Calculate mean csMPa for rows with 0 in these columns\n",
    "                self.data.loc[mask, 'csMPa'] = mean_csMPa_for_zeros  # Assign mean csMPa to those rows\n",
    "                self.data[col].fillna(self.data[col].mean(), inplace=True)  # Fill remaining NaNs with mean\n",
    "        elif method == 'median':\n",
    "            self.data.fillna(self.data.median(), inplace=True)\n",
    "        elif method == 'knn':\n",
    "            imputer = KNNImputer()\n",
    "            self.data = pd.DataFrame(imputer.fit_transform(self.data), columns=self.data.columns)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid imputation method.\")\n",
    "\n",
    "        # Apply transformations (vectorized for efficiency)\n",
    "        self.data[['age', 'cement', 'water', 'fineaggregate', 'coarseaggregate']] = np.log(\n",
    "            self.data[['age', 'cement', 'water', 'fineaggregate', 'coarseaggregate']]\n",
    "        )  # Apply log transformation to specified columns, including 'age'\n",
    "\n",
    "        self.data[['cement', 'superplasticizer']] = np.sqrt(\n",
    "            self.data[['cement', 'superplasticizer']]\n",
    "        )  # Apply square root transformation to specified columns\n",
    "\n",
    "\n",
    "\n",
    "        return self.data  # Return the preprocessed DataFrame\n",
    "\n",
    "    def handle_outliers(self, method='capping', threshold=3):\n",
    "        \"\"\"Handles outliers using the specified method.\"\"\"\n",
    "        if method == 'capping':\n",
    "            for col in self.data.columns:\n",
    "                self.data[col] = self.data[col].clip(lower=self.data[col].quantile(0.05),\n",
    "                                                    upper=self.data[col].quantile(0.95))\n",
    "        elif method == 'winsorizing':\n",
    "            for col in self.data.columns:\n",
    "                IQR = self.data[col].quantile(0.75) - self.data[col].quantile(0.25)\n",
    "                if IQR != 0:\n",
    "                    lower_bound = self.data[col].quantile(0.25) - threshold * IQR\n",
    "                    upper_bound = self.data[col].quantile(0.75) + threshold * IQR\n",
    "                    self.data[col] = self.data[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "                else:\n",
    "                    print(f\"IQR for {col} is zero, skipping winsorizing.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid outlier handling method.\")\n",
    "\n",
    "\n",
    "        return self.data  # Return the preprocessed DataFrame\n",
    "\n",
    "        \n",
    "    def removeNaN(self) -> pd.DataFrame:\n",
    "        \"\"\"Removes rows with missing values and applies transformations.\"\"\"\n",
    "        # Drop rows with missing values\n",
    "        self.data.dropna(inplace=True)\n",
    "\n",
    "        # Apply log transformation to specified columns (after removing NaNs)\n",
    "        for variable in self.data.columns[:-1]:\n",
    "            if variable in ['cement', 'water', 'age', 'fineaggregate', 'coarseaggregate']:\n",
    "                self.data[variable] = np.log(self.data[variable])\n",
    "\n",
    "        return self.data  # Return the preprocessed DataFrame\n",
    "\n",
    "# Example usage\n",
    "data = PreProcessing(filename)\n",
    "concdata = data.FillNaN(method='mean')  # Fill with median\n",
    "#print(data)\n",
    "#concretedata = data.handle_outliers(method= 'capping')  # Handle outliers using capping\n",
    "concdata = data.removeNaN()  # Remove rows with missing values\n",
    "print(concdata.head(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aakash's Preprocessing - by inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If values before and after are the same, this value replaces the NaN. No problems with this.\n",
    "\n",
    "If values before and after are different, things get messy. There are three scenarios:\n",
    "\n",
    "1. Needs to be filled with the value before. Most frequent scenario. Example from database is water row 16; looking at excel file, the NaN should obviously be filled with 228. This is done in the code below (1).\n",
    "\n",
    "2. Needs to be filled with mean. Example from database is age row 4; looking at excel file, it's not obvious what the NaN should be filled with, so we'll use the average of the two before and after (can this be changed to something better?). This is done in the code below (2).\n",
    "\n",
    "3. Needs to be filled with the value after. Least frequent scenario, there are 5 of these scenarios. Example from database is slag row 185 and 186. This HAS NOT been implemented into the code, I'm not sure what conditions would allow for some NaNs to be replaced with values before and some with values after. Because there are only 5, does this matter that much?\n",
    "\n",
    "THIS CODE IS REALLY MESSY AND I APOLOGISE TO YOU THREE GOOD CODERS WHO MAY HAVE A STROKE SEEING WHAT I'VE DONE. i just could not get the fillna method to work for the \"one and two before\" method (if you ask copilot this is what it recommends using) so this is something i'm sure one of you will be able to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InspPreProcessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.variables = variables\n",
    "\n",
    "    def rename_columns(self):\n",
    "        self.df.rename(columns=dict(zip(self.df.columns, variables)),\n",
    "            inplace=True)\n",
    "        return self.df\n",
    "    \n",
    "    def replaceNaN(self) -> pd.DataFrame:\n",
    "\n",
    "        # for loops - looping over the whole dataframe\n",
    "        for col in self.df.columns:\n",
    "            for i in range(1, len(self.df) - 1):\n",
    "                \n",
    "                # concerning NaN values\n",
    "                if pd.isna(self.df[col].iloc[i]):\n",
    "\n",
    "                    # if the values BEFORE AND AFTER are DIFFERENT\n",
    "                    if self.df[col].iloc[i-1] != self.df[col].iloc[i+1]:\n",
    "\n",
    "                        # 1 - if the values ONE AND TWO BEFORE are the SAME\n",
    "                        if self.df[col].iloc[i-1] == self.df[col].iloc[i-2]:\n",
    "                            self.df[col].iloc[i] = self.df[col].iloc[i-1] # fill with before value\n",
    "                        \n",
    "                        # 2 - if the values ONE AND TWO BEFORE are DIFFERENT\n",
    "                        else:\n",
    "                            self.df[col].iloc[i] = (self.df[col].iloc[i-1] + self.df[col].iloc[i+1]) / 2 # fill with average - NEEDS CHANGING\n",
    "                \n",
    "                    # if the values BEFORE AND AFTER are the SAME\n",
    "                    else:\n",
    "                        self.df[col].iloc[i] = self.df[col].iloc[i-1] \n",
    "\n",
    "        return self.df\n",
    "    \n",
    "df = pd.read_csv(\"Concrete_Data_Yeh_final.csv\")\n",
    "variables = ['cement', 'slag', 'ash', 'water', 'superplastic','coarseagg','fineagg', 'age', 'strength']\n",
    "\n",
    "preprocessor = InspPreProcessing(df)\n",
    "\n",
    "concdata = preprocessor.rename_columns()\n",
    "concdata = preprocessor.replaceNaN()\n",
    "\n",
    "concdata.head(190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Preprocessing - by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c4ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPreProcessing:\n",
    "    \"Replacing all NaN values in the original database\"\n",
    "    \"with mean values of the corresponding variable\"\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.variables = variables\n",
    "    \n",
    "    def renameColumns(self):\n",
    "        #simple column names\n",
    "        self.df.rename(columns=dict(zip(self.df.columns, variables)), \n",
    "            inplace=True)\n",
    "        return self.df\n",
    "\n",
    "    def replaceNaN(self) -> pd.DataFrame:\n",
    "        #replace NaN values with mean values\n",
    "        mean_values = self.df.mean()\n",
    "        self.df.fillna(value=mean_values, inplace=True)\n",
    "        return self.df\n",
    "    \n",
    "    def meanValues(self) -> pd.DataFrame:\n",
    "        mean_values = self.df.mean()\n",
    "        mean_df = pd.DataFrame(mean_values, columns=['mean'])\n",
    "        return mean_df\n",
    "\n",
    "df = pd.read_csv(\"Concrete_Data_Yeh_final.csv\")\n",
    "variables = ['cement', 'slag', 'ash', 'water', 'superplastic','coarseagg','fineagg', 'age', 'strength']\n",
    "\n",
    "preprocessor = MeanPreProcessing(df)\n",
    "\n",
    "concdata = preprocessor.renameColumns()\n",
    "concdata = preprocessor.replaceNaN()\n",
    "mean_df = preprocessor.meanValues()\n",
    "\n",
    "mean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concdata.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = concdata.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5495d9",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f5f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "trainconcdata, testconcdata = train_test_split(concdata, test_size=0.2, random_state=42)\n",
    "\n",
    "scaled_trainconcdata = pd.DataFrame(scaler.fit_transform(trainconcdata), columns=variables)\n",
    "scaled_testconcdata = pd.DataFrame(scaler.transform(testconcdata), columns=variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RegressionModel(regression_type, regression_name, metriclist, xaxis=xaxis):\n",
    "    #observing linear and ridge regression models for each variable\n",
    "    linearheatmap = {}\n",
    "    ridgeheatmap = {}\n",
    "\n",
    "    for index, i in enumerate(scaled_trainconcdata.columns[:-1]):\n",
    "\n",
    "        x_test = scaled_testconcdata[i].to_numpy().reshape(-1,1)\n",
    "        y_test = scaled_testconcdata['strength'].to_numpy().reshape(-1,1)\n",
    "        x_train = scaled_trainconcdata[i].to_numpy().reshape(-1,1)\n",
    "        y_train = scaled_trainconcdata['strength'].to_numpy().reshape(-1,1)\n",
    "\n",
    "        regressor = regression_type()\n",
    "        regressor.fit(x_train, y_train)\n",
    "        y_pred = regressor.predict(x_test)\n",
    "        linearheatmap[i] = float(regressor.coef_)\n",
    "        ridgeheatmap[i] = float(regressor.coef_)\n",
    "        \n",
    "        metrics = []\n",
    "        for j in metriclist:\n",
    "            metrics.append((j.__name__, j(y_test, y_pred)))\n",
    "        print (pd.DataFrame(metrics, columns=['Metric', 'Value']))\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.scatter(x_test, y_test, color='blue', label='Actual', s=5)\n",
    "        plt.plot(x_test, y_pred, color='red', label='Predicted')\n",
    "        plt.title(f'{regression_name} | Score = {format(regressor.score(x_test, y_test), \".3f\")} | Gradient = {format(regressor.coef_[0][0], \".3f\")}')\n",
    "        plt.xlabel(xaxis[index])\n",
    "        plt.ylabel('Compression Strength (MPa)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return linearheatmap, ridgeheatmap\n",
    "\n",
    "def Heatmap(heatmap, regression_name):\n",
    "    #creating heatmap for each variable for each regression model\n",
    "    heatmap_df = pd.DataFrame(heatmap, index=[\"strength\"])\n",
    "    plt.figure(figsize=(8, 1))\n",
    "    sns.heatmap(heatmap_df, cmap='coolwarm', annot=True, annot_kws={'size': 12})\n",
    "    plt.title(f'{regression_name} | Heat map')\n",
    "    plt.show()\n",
    "\n",
    "metriclist = [explained_variance_score, mean_absolute_error, mean_squared_error, r2_score]\n",
    "xaxis = ['Cement (kg/m3)', 'Blast Furnace Slag (kg/m3)', 'Fly Ash (kg/m3)', 'Water (kg/m3)', 'Superplasticizer (kg/m3)', 'Coarse Aggregate (kg/m3)', 'Fine Aggregate (kg/m3)', 'Age (days)']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearheatmap = RegressionModel(LinearRegression, \"Linear Regression\", metriclist, xaxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeheatmap = RegressionModel(Ridge, \"Ridge Regression\", metriclist, xaxis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmap(linearheatmap[0], \"Linear Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Heatmap(ridgeheatmap[0], \"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = concdata.iloc[:,:-1].to_numpy() \n",
    "#y = concdata['csMPa'].to_numpy()  # IF USING MERVYN PREPROCESSING\n",
    "y = concdata[\"strength\"].to_numpy()  # IF USING AAKASH PREPROCESSING\n",
    "\n",
    "print(f'y:\\n{y}\\n')\n",
    "print(f'X:\\n{pd.DataFrame(X)}\\n') # to check if the data is split correctly (X = 2D array, y = 1D array).\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "class RegressionTypes:\n",
    "    \"Observing correlation between predicted and actual values\"\n",
    "    \"of compressive strength for each regression model\"\n",
    "    def __init__(self, x_train, x_test, y_train, y_test, regression, **kwargs): #input if needed: metriclist = list\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.regression = regression(**kwargs)\n",
    "        self.regfit = self.regression.fit(x_train, y_train)\n",
    "        self.y_pred = self.regression.predict(x_test)\n",
    "        # self.y_pred is a numpy array that gives a predicted value of y for each x in x_test, given the fit of the trained model.\n",
    "    \n",
    "    def Metric(self, metric, **kwargs):\n",
    "        #scores = cross_val_score(self.regression, self.x_train, self.y_train, cv=5)\n",
    "        #print(metric(self.y_test, self.y_pred))\n",
    "        #print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        return metric(self.y_test, self.y_pred), self.regression.score(self.x_test, self.y_test)\n",
    "\n",
    "    def Plot(self, **kwargs):\n",
    "        ref_x = np.linspace(0, 80, 1000)\n",
    "        ref_y = ref_x\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(self.y_test, self.y_pred, color='red', s=5)\n",
    "        plt.title(f'Score: {self.regression.score(self.x_test, self.y_test)}')\n",
    "        plt.plot(ref_x, ref_y, color='black', linestyle = 'dashed')\n",
    "        plt.xlabel('Actual compressive strength (Mpa)')\n",
    "        plt.ylabel('Predicted compressive strength (Mpa)')\n",
    "        plt.show()\n",
    "        return\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [LinearRegression, Ridge, Lasso, RandomForestRegressor]\n",
    "\n",
    "metrics = [lambda y_true, y_pred: mean_squared_error(y_true, y_pred, squared=False), mean_squared_error, r2_score, mean_absolute_error, explained_variance_score]\n",
    "\n",
    "for i, r in enumerate(regressors):\n",
    "    print(f'Using {r.__name__}')\n",
    "    if i == 0:\n",
    "        RegressionTypes(x_train, x_test, y_train, y_test, r).Plot()\n",
    "    elif i in [1, 2]:\n",
    "        RegressionTypes(x_train, x_test, y_train, y_test, r, alpha = 0.1, random_state = 42).Plot()\n",
    "    else:\n",
    "        RegressionTypes(x_train, x_test, y_train, y_test, r, n_estimators = 200, max_depth = 30, random_state = 42).Plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "these regression scores are pretty bad (except random forest <3). need to get them up if we want to use this form of pre-processing (which i do think is the best personally).\n",
    "\n",
    "mervyn's preprocessing still displays the \"input contains NaN or infinity\" error for me. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
