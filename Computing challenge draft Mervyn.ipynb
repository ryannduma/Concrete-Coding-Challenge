{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a84ef06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68bf15",
   "metadata": {},
   "source": [
    "## Notes to consider for the challenge\n",
    "\n",
    "1. Pre-process the data is somewhat complete, though we may need to confirm between the group to check if this is the best representation of data.\n",
    " - We found the mean of each column from the data base (Concrete_Data_Yeh_final.csv) - but is this the right approach?\n",
    "\n",
    "2. Try to do a function that goes through all types of regression and format a demo plot to see if this regression is appropriate for one column.\n",
    " - If results are satisfactory, we need to firm this type of regression for each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3abcbb77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281.1678640776696 104.50636449481543 1030\n",
      "73.66865234374998 85.97389236764685 1030\n",
      "54.122837706511156 63.962457657715774 1030\n",
      "181.50626223091973 21.039881144231543 1030\n",
      "6.219881889763779 5.925527321572695 1030\n",
      "972.6539589442809 77.65912972100936 1030\n",
      "773.6981499513134 79.89686334327651 1030\n",
      "45.42731707317073 62.324091303247805 1030\n",
      "35.81796116504851 16.705741961912505 1030\n"
     ]
    }
   ],
   "source": [
    "filename = 'Concrete_Data_Yeh_final.csv'\n",
    "variables = ['cement', 'slag', 'flyash', 'water', 'superplasticizer', 'coarseaggregate', 'fineaggregate', 'age', 'csMPa']\n",
    "\n",
    "class PreProcessing:\n",
    "    \"The aim of this class is to successfully replace all the empty values from\"\n",
    "    \"the file given, and to split the columns into their own series / arrays.\"\n",
    "    def __init__(self, file):\n",
    "        self.data = pd.read_csv(file)\n",
    "        \n",
    "    def FillNaN (self):\n",
    "        # Here we used the .mean method to replace the NaN values from the original dataset. \n",
    "        # (Please check this as we may use an alternative fillna method)\n",
    "        for i in self.data.columns:\n",
    "            self.data[i].fillna(self.data[i].mean(), inplace = True)\n",
    "            print(self.data[i].mean(), self.data[i].std(), len(self.data[i]))\n",
    "        return self\n",
    "    \n",
    "    #def SplitColumns (self, variablelist)-> dict:      \n",
    "        # not sure what to do here, trying to split each column in the dataframe to their individual series / arrays. (To sort out)\n",
    "        #Dict = {}\n",
    "        #for variable in variablelist:\n",
    "            #Dict[variable] = self.data[variable].to_numpy()\n",
    "        #return Dict\n",
    "\n",
    "    def SplitColumns (self, variable) -> np.array:\n",
    "        return self.data[variable].to_numpy()\n",
    "\n",
    "test = PreProcessing(filename).FillNaN()\n",
    "# Here, we will form global variables for each array. These can be recalled for scaling and modelling, however, it cannot visually show which variable its from unless specified in further code. \n",
    "for i, variable in enumerate(variables):\n",
    "    globals()[variable] = test.SplitColumns(variables[i])\n",
    "\n",
    "# just for show\n",
    "#print(cement, '\\n slag:', slag, '\\n flyash:', flyash, '\\n water:', water, '\\n superplasticizer:', superplasticizer, \n",
    "# '\\n coarseaggregate:', coarseaggregate, '\\n fineaggregate:', fineaggregate, '\\n age:', age, '\\n csMPa', csMPa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e457e",
   "metadata": {},
   "source": [
    "### Scaling the Data\n",
    "We will scale variables here to understand and have easier comparison between variables, as they will be all under the same axis. It is better to scale such data before performing regression or any other machine learning algorithms that may follow.\n",
    "Extra: I have made a simple plot to help understand the scaling - it will range between 0 and 1 using the MinMaxScaler from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22b7bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07417582]\n",
      " [0.07417582]\n",
      " [0.73901099]\n",
      " ...\n",
      " [0.07417582]\n",
      " [0.07417582]\n",
      " [0.07417582]] [[0.96748474]\n",
      " [0.74199576]\n",
      " [0.47265479]\n",
      " ...\n",
      " [0.26622649]\n",
      " [0.37922013]\n",
      " [0.37461069]]\n"
     ]
    }
   ],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "def BasicGraph(x_variable, x_name, y_name = 'Compressive Stength (MPa)', y_variable = csMPa):\n",
    "    plt.scatter(x_variable, y_variable, marker='o', s=1)\n",
    "    #plt.scatter(x_variable, y_variable, marker='o', s=10)\n",
    "    ax.set_xlabel(f'{x_name}')\n",
    "    ax.set_ylabel(f'{y_name}')\n",
    "    ax.set_title(f'Relationship between {y_name} and {x_name}')\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def MinMaxScaling(ivariable,ivariablename, dvariablename = 'csMPa', dvariable = csMPa):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    plt.scatter(scaler.fit_transform(ivariable.reshape(-1,1)), scaler.fit_transform(dvariable.reshape(-1,1)), marker='o', s=10)\n",
    "    ax.set_xlabel(f'{ivariablename}')\n",
    "    ax.set_ylabel(f'{dvariablename}')\n",
    "    ax.set_title(f'MinMax Scaling of Variables: {dvariablename} and {ivariablename}')\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "def MinMaxScaling2(variable):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(variable.reshape(-1,1))\n",
    "#def StandardScaling(ivariable,ivariablename, dvariablename = 'csMPa', dvariable = csMPa):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    plt.scatter(scaler.fit_transform(ivariable.reshape(-1,1)), scaler.fit_transform(dvariable.reshape(-1,1)), marker='o', s=10)\n",
    "    ax.set_xlabel(f'{ivariablename}')\n",
    "    ax.set_ylabel(f'{dvariablename}')\n",
    "    ax.set_title(f'Standard Scaling of Variables: {dvariablename} and {ivariablename}')\n",
    "    #plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "agescaled = MinMaxScaling2(age)\n",
    "csMPascaled = MinMaxScaling2(csMPa)\n",
    "print(agescaled, csMPascaled)\n",
    "#test2 = BasicGraph(flyash, variables[2])\n",
    "#test3 = StandardScaling(slag, variables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7219fa22",
   "metadata": {},
   "source": [
    "### Building the Regression Model\n",
    "Now that we got all the variables split into individual arrays (and more importantly scaled!), this will be processed such that:\n",
    "1. Each dependant variable and csMPa (output variable) will be split into a training and test set - the training ratio can change depending on variance vs bias.\n",
    "2. Will go through each type of regression technique (Linear, Ridge, Lasso and ElasticNet) via .fit() on the training data - this will determine which regression technique is best. To support the fit, .coef_ and .intercept are used as well on the training sets. The test set will undergo the .score() and .predict() functions to be applied on the training data.\n",
    "3. To visually understand what we are dealing with, we will plot a demo graph to examine how well our information can provide the most accurate observation. We repeat this for each type of regression -  ideally, the higher the .score() value + good spread of .predict(), the better the final representation when making the interactive graph.\n",
    " \n",
    "To Note: The original x and y data (x and y) will be saved for future reference if they need to be used, e.g. plotting the graphs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c30817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Lasso(), array([0.56840082]), array([0.]), array([0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082, 0.56840082,\n",
      "       0.56840082, 0.56840082, 0.56840082, 0.56840082]), -1.0259371903440306)\n"
     ]
    }
   ],
   "source": [
    "class RegressionModel:\n",
    "    def __init__(self, trainratio, regressor, dvariable, ivariable = csMPascaled):\n",
    "        'Setup of input variables for functions used below in the Regression model'\n",
    "        self.dvariable = dvariable\n",
    "        self.ivariable = ivariable\n",
    "        self.trainratio = trainratio\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def TrainandTest(self):\n",
    "        'Separates the data to a training and test set'\n",
    "        # get training data\n",
    "        n_rows = int(len(self.ivariable) * self.trainratio)\n",
    "\n",
    "        # do split\n",
    "        self.dtrain = self.dvariable[:n_rows]\n",
    "        self.dtest = self.dvariable[n_rows:]\n",
    "        self.itrain = self.ivariable[:n_rows]\n",
    "        self.itest = self.ivariable[n_rows:]\n",
    "            \n",
    "        # reshape x to ensure it is 2D\n",
    "        #trained_x = dtrain.reshape(-1,1)\n",
    "        #test_x = dtest.reshape(-1,1)\n",
    "            \n",
    "        return self.dtrain, self.dtest, self.itrain, self.itest #We can merge this definition with the Regression definition below if necessary.\n",
    "    \n",
    "    def Regression(self):\n",
    "        'Uses the regression model inputted such that it uses:'\n",
    "        'Training set: fit, coefficients and intercept'\n",
    "        'Test set: predictivity and score (explained variance, mean absolute error, mean squared error, r2 score may be also implemented)'\n",
    "        \n",
    "        #Inheriting the attributes from the TrainandTest def\n",
    "        self.dtrain, self.dtest, self.itrain, self.itest = self.TrainandTest()\n",
    "\n",
    "        fit = self.regressor.fit(self.dtrain.reshape(-1,1), self.itrain)\n",
    "        self.intercept = self.regressor.intercept_\n",
    "        self.coefficients = self.regressor.coef_\n",
    "        self.predictivity = self.regressor.predict(self.dtest.reshape(-1,1))\n",
    "        self.score = self.regressor.score(self.dtest.reshape(-1,1), self.itest)\n",
    "\n",
    "        return fit, self.intercept, self.coefficients, self.predictivity, self.score\n",
    "    \n",
    "    def DemoPlot(self, dname, iname = 'csMPa'):\n",
    "        'This is a demo plot to show the regression model'\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.scatter(self.dtest, self.predictivity, marker='o', s=10)\n",
    "        plt.plot(self.dtest, self.itest, color='red', linewidth=1)\n",
    "        ax.set_xlabel(f'{dname}')\n",
    "        ax.set_ylabel(iname)\n",
    "        ax.set_title(f'Relationship between {iname} and {dname}, score = {self.score:.2f}')\n",
    "        plt.show()\n",
    "        return fig, ax\n",
    "    \n",
    "# As a note: If results are not buying, changing 1 of the variables to a logarithmic scale may help.\n",
    "# It all depends on the general trend provided by the Regession model.\n",
    "    \n",
    "test5 = RegressionModel(0.2, Lasso(), agescaled).Regression()\n",
    "print(test5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15ce61",
   "metadata": {},
   "source": [
    " Now that the test and training sets are made, we can start applying our regressors, to find which one is best for the model. To ensure the best observed result, we will use .fit/.pred, followed by .score using mean, range, and especially r2 score to determine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077faa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class to compare different regression techniques\n",
    "class Regressor:\n",
    "    \"\"\"Container for analysing different metrics for a single regression class\"\"\"\n",
    "    def __init__(self, regressmode, corrected_data, variablelist = list, trainratio = float, **kwargs):\n",
    "        # construct regressor object\n",
    "        self.regressor = regressmode(**kwargs)\n",
    "        \n",
    "        # use load function via inheritance\n",
    "        self.X, self.y, self.X_train, self.y_train, self.X_test, self.y_test = trainandtest(corrected_data, variablelist, trainratio)\n",
    "        \n",
    "        # fit data\n",
    "        self.regressor.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # get predicted data\n",
    "        self.y_pred = self.regressor.predict(self.X_test)\n",
    "        \n",
    "    def metric(self, regressmode, **kwargs) -> float:\n",
    "        \"\"\"Takes a sklearn.metrics class and returns the score of the regressor object\"\"\"\n",
    "        \n",
    "        # use the metric class to get a score\n",
    "        return regressmode(self.y_test, self.y_pred)\n",
    "\n",
    "# create a list of regressors\n",
    "ListofRegressors = [LinearRegression, Ridge, Lasso, ElasticNet, RandomForestRegressor]\n",
    "MeasureScoreList = [explained_variance_score, mean_absolute_error, mean_squared_error, r2_score]\n",
    "\n",
    "# Calling out the function commands via a loop\n",
    "for i, reg in enumerate(ListofRegressors):\n",
    "    \n",
    "    # intialise regressor\n",
    "    regressor = Regressor(reg, b, Variables, 0.2)\n",
    "    for j, metric in enumerate(MeasureScoreList):\n",
    "        score = regressor.metric(metric)\n",
    "        print(score)\n",
    "\n",
    "# Error somewhere here - the loop is not working? Need help with diagnosing the probem. Maybe ask Copilot for assistance for further guidance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
